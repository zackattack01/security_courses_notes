# Running Controlled Experiments

#### Steps
- state a hypothesis
- identify independent and dependent vars
- design the experimental protocol
- choose the user group for testing
- run it with some pilot experiments 
- fix protocol from these results
- run the real experiment
- statistical analysis
- draw conclusions
- tell some people

## Usability Studies

- Important to realize that good security shouold be a seamless part of another process
- When listing tasks, they shouldn't be asking questions, they should be presenting things that the user might go to the site to do
- Users shouldn't be wondering what to do, they should know what is being asked of them and should be able to figure out how to do it from the site
  - the tasks shouldn't be providing instructions (a user wouldn't come to the site to follow instructions)
- Choose representative tasks covering the breadth of things that users might do on the site
  - highlight the most important aspects
- If someone wants to check their bank account they might need to sign in, and this step should be easy and intuitive,but it isn't a task by itself
- Put background questions on your pre test questionnaire
- Flow should go: fill out pre questions → recieve tasks → evaluate by completing tasks → post task questionnaire
- For a silent observer analysis you would just watch as the user completes the task and not say anything
- For the think-aloud method the user talks through what they're doing or thinking.  this is useful because users often do bizarre things that you would never expect
- For a constructive interaction method: 
  - there is a give and take between the subject and participating experimenter
- After the experiment there is typically a post-test questionnaire, and sometimes an interview can be even more useful as users wont want to write out a detailed explanation of what happened
- When reporting
  - Summarize the findings with quotes and specific but representative examples of what may have happened
  - Suggest improvements that could be made for usability based on the findings

## A/B Testing
- Show users two or more options, and see what works by measuring the performance
- Examples of things that could be changed from one option to another
  - text on a button (example given was changing button from 'sign up' to 'learn more')
- Disadvantage of this type of testing is often that you have no idea why one option performed better than another
- How:
  - start wih a small percentage of visitors trying the experimental conditions
  - automatically stop testing if any condition that has bad performance
  - let people consistently see the same option, don't change it for them in between visits
  - use google analytics (this will also allow you to ensure that users will be shown the same option each time they come to the site)
